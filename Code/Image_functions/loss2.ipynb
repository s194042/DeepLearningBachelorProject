{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#from torch import nn\n",
    "#import numpy as np\n",
    "#from torchsummary import summary\n",
    "#import Lossv2\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#summary(Lossv2.Loss().to(device), (6, 768, 512))  ###  IDK why, but I have to switch the dims in AvgPool on Loss() to get this to work :/ But that breaks the stuff bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "torch.manual_seed(100)\n",
    "np.random.seed(100)\n",
    "import Lossv2\n",
    "import generateLossImages\n",
    "import time\n",
    "\n",
    "global counter \n",
    "counter = 40\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def save_ckp(state, is_best, checkpoint_dir=\"./models/rest/\", best_model_dir=\"./models/best/\"):\n",
    "    global counter \n",
    "    f_path = checkpoint_dir + str(counter) + '_checkpoint.pt'\n",
    "    counter = counter + 1\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_dir + 'best_model.pt'\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "\n",
    "def load_ckp(model, optimizer, checkpoint_fpath=\"./models/rest/15_checkpoint.pt\"):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch'], checkpoint['index'], checkpoint['min_lr'], checkpoint['max_lr'], checkpoint['steps'], checkpoint['step_size'], checkpoint['falling'], checkpoint['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "loss_fn = nn.L1Loss(reduction='sum')\n",
    "min_lr = 0.001\n",
    "max_lr = 0.004\n",
    "decay = 0.9\n",
    "steps = 500\n",
    "falling = True\n",
    "start_epoch = 0\n",
    "start_index = 0\n",
    "momentum = 0.94\n",
    "threshold = [0.16, 0.12, 0.09, 0.06, 0.04, 0.03, 0.02, 0.015, 0.01 -1]\n",
    "step_size = (max_lr-min_lr)/steps\n",
    "model = Lossv2.Loss(seperable=True, slim=True).to(device).to(memory_format=torch.channels_last)\n",
    "#x = torch.rand(22, 8, 512, 768).to(device).to(memory_format=torch.channels_last)\n",
    "model = torch.jit.script(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=max_lr, momentum=momentum)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "#\n",
    "#torch.autograd.set_detect_anomaly(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "model = Lossv2.Loss(seperable=True, slim=True).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=max_lr, momentum=momentum)\n",
    "model, optimizer, start_epoch, start_index, min_lr, max_lr, steps, step_size, falling, threshold = load_ckp(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#li = [31.007527351379395, 0.01501011848449707, 0.0010006427764892578, 28.98952603340149, 0.48702239990234375, 47.146605014801025, 12.32689380645752, 0.5665144920349121]\n",
    "#su = sum(li)\n",
    "#print([100*l/su for l in li])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rani\\AppData\\Local\\Temp\\ipykernel_4464\\744408567.py:17: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  torch.autograd.detect_anomaly(check_nan=False)\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "printing = True\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "lis = []\n",
    " # might have to delete when loading model\n",
    "threshold_decay = 0.2\n",
    "flags = [True for _ in threshold]\n",
    "record = 1\n",
    "epochs = 100\n",
    "times = [0]*8\n",
    "batch_size= 12\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.autograd.detect_anomaly(check_nan=False)\n",
    "torch.autograd.set_detect_anomaly(False, check_nan=False)\n",
    "torch.autograd.profiler.profile(enabled=False)\n",
    "torch.autograd.profiler.emit_nvtx(enabled=False)\n",
    "#torch.autograd.gradgradcheck(check_undefined_grad= False)\n",
    "#torch.autograd.gradcheck(check_undefined_grad= False)\n",
    "#torch.no_grad()\n",
    "\n",
    "scaler = GradScaler()\n",
    "#import warnings \n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 53\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m#load = time.time()\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[39m#times[1] += load-start\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[39m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 53\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     55\u001b[0m     \u001b[39m#out = time.time()\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[39m#times[3] += (out-lab)\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m     \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\Rani\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "startup = False\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    if startup:\n",
    "        training = generateLossImages.MakeIter(start_index=start_index if epoch == start_epoch else 0, startup = True)#, epoch=epoch) # start=2*71*50*4))#start_index if epoch == start_epoch else 0)) \n",
    "        #val = generateLossImages.get_image_pairs_transforms_with_loss(\"C:/Users/Rani/Desktop/ai_val/16\")\n",
    "        training_loader = torch.utils.data.DataLoader(training, batch_size=batch_size, num_workers=2)#), worker_init_fn=worker_init_fn) # num_workers\n",
    "        startup = True\n",
    "        min_lr *= batch_size**0.5\n",
    "        max_lr *= batch_size**0.5\n",
    "        step_size = (max_lr-min_lr)/steps\n",
    "        optimizer.param_groups[-1]['lr'] = max_lr\n",
    "        los = [0]*(112//batch_size)\n",
    "        pred = [0]*(112//batch_size)\n",
    "        \n",
    "    else:\n",
    "        training = generateLossImages.MakeIter(start_index=start_index if epoch == start_epoch else 0, epoch=epoch, startup = False) # start=2*71*50*4))#start_index if epoch == start_epoch else 0)) \n",
    "        #val = generateLossImages.get_image_pairs_transforms_with_loss(\"C:/Users/Rani/Desktop/ai_val/16\")\n",
    "        training_loader = torch.utils.data.DataLoader(training, batch_size=batch_size, num_workers=2)#2)#), worker_init_fn=worker_init_fn) # num_workers\n",
    "        #max_lr /= 10\n",
    "        #min_lr /= 10\n",
    "        #steps *= 2\n",
    "        #step_size = (max_lr-min_lr)/steps\n",
    "        #falling = True\n",
    "        #optimizer.param_groups[-1]['lr'] = max_lr\n",
    "\n",
    "    #end = 0\n",
    "    #before_start = 0\n",
    "    #in_upstart = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index, data in enumerate(training_loader):#[90.06174230575562, 0.03202486038208008, 0.016014575958251953, 84.36967897415161, 1.3647308349609375, 137.4005196094513, 34.81299662590027, 1.9427943229675293, 0, 0]\n",
    "        #print(len(inputs))\n",
    "        #before_start += 1\n",
    "        #if before_start > 55*11/batch_size and in_upstart:\n",
    "        #    in_upstart = False\n",
    "        #    times = [0]*8\n",
    "        #start = time.time()\n",
    "        #if end != 0:\n",
    "        #    times[0] += (start-end)\n",
    "        inputs, labels = data\n",
    "        labels = torch.unsqueeze(labels, dim=-1)\n",
    "        inputs.to(memory_format=torch.channels_last)\n",
    "        #load = time.time()\n",
    "        #times[1] += load-start\n",
    "\n",
    "        #lab = time.time()\n",
    "        #times[2] += (lab-load)\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #out = time.time()\n",
    "            #times[3] += (out-lab)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            #los = time.time()\n",
    "            #times[4] += (los-out)\n",
    "\n",
    "        loss.backward()#scaler.scale(loss).backward()#loss.backward()\n",
    "        \n",
    "        #back = time.time()\n",
    "        #times[5] += (back-los)\n",
    "        # Gather data and report\n",
    "        if startup:\n",
    "            los[index%(112//batch_size)] = loss.item()\n",
    "            pred[index%(112//batch_size)] = (list(zip(labels.detach().cpu().numpy().tolist(),outputs.detach().cpu().numpy().tolist())))\n",
    "            \n",
    "        #runn = time.time()\n",
    "        #times[6] += (runn-back)\n",
    "        if startup:\n",
    "            if index % (112//batch_size) == (112//batch_size)-1:\n",
    "                optimizer.step()#scaler.step(optimizer)#optimizer.step()\n",
    "                #scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            optimizer.step()#scaler.step(optimizer)#optimizer.step()\n",
    "                #scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        if falling:\n",
    "            optimizer.param_groups[-1]['lr'] = optimizer.param_groups[-1]['lr'] - step_size\n",
    "            if optimizer.param_groups[-1]['lr'] < min_lr:\n",
    "                falling = False\n",
    "                max_lr *= decay\n",
    "                min_lr *= decay\n",
    "                steps /= decay\n",
    "                step_size = (max_lr-min_lr)/steps\n",
    "\n",
    "                if printing:\n",
    "                    print(\"!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                    print(min_lr)\n",
    "                    print(max_lr)\n",
    "                    print(steps)\n",
    "                    print(step_size)\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'index': index,\n",
    "                    'min_lr': min_lr,\n",
    "                    'max_lr': max_lr,\n",
    "                    'steps': steps,\n",
    "                    'step_size': step_size,\n",
    "                    'falling': falling,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'threshold': threshold\n",
    "                }\n",
    "                save_ckp(checkpoint, True)\n",
    "                \n",
    "                if startup and sum(los)/10 < 0.05 and max(los) < 0.1:\n",
    "                    #max_lr *= 0.4\n",
    "                    #min_lr *= 0.4\n",
    "                    #steps /= 0.4\n",
    "                    #step_size = (max_lr-min_lr)/steps\n",
    "                    #print(\"loss\")\n",
    "                    print(loss.item())\n",
    "                    print(\"lr\")\n",
    "                    print(optimizer.param_groups[-1]['lr'])\n",
    "                    print(\"pred\")\n",
    "                    print(labels)\n",
    "                    print(outputs.T)\n",
    "                    startup = False\n",
    "                    max_lr /= 4\n",
    "                    min_lr /= 4\n",
    "                    steps *= 2\n",
    "                    step_size = (max_lr-min_lr)/steps\n",
    "                    falling = True\n",
    "                    optimizer.param_groups[-1]['lr'] = max_lr\n",
    "                    print(\"startup done !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        else: \n",
    "            optimizer.param_groups[-1]['lr'] += step_size\n",
    "            if optimizer.param_groups[-1]['lr'] > max_lr:\n",
    "                falling = True\n",
    "\n",
    "        if startup:\n",
    "            if printing and index % (112//batch_size) == (112//batch_size)-1:\n",
    "                print(\"loss\")\n",
    "                print(loss.item())\n",
    "                print(\"lr\")\n",
    "                print(optimizer.param_groups[-1]['lr'])\n",
    "                print(\"pred\")\n",
    "                print(pred)\n",
    "                \n",
    "        elif index % (20*(112//batch_size)-1) == 0:\n",
    "        #    print(\"loss\")\n",
    "            print(\"loss\")\n",
    "            print(loss.item())\n",
    "            print(\"lr\")\n",
    "            print(optimizer.param_groups[-1]['lr'])\n",
    "            print(\"pred\")\n",
    "            print(labels.T)\n",
    "            print(outputs.T)\n",
    "        #        print('  batch {} loss: {}'.format(index + 1, last_loss))\n",
    "        #        print(lis)\n",
    "        #        lis = []\n",
    "        #    running_loss = 0\n",
    "\n",
    "        #    if last_loss < record:\n",
    "        #        record = last_loss\n",
    "        #        if last_loss < 0.08:\n",
    "        #            checkpoint = {\n",
    "        #            'epoch': epoch,\n",
    "        #            'index': index,\n",
    "        #            'min_lr': min_lr,\n",
    "        #            'max_lr': max_lr,\n",
    "        #            'steps': steps,\n",
    "        #            'step_size': step_size,\n",
    "        #            'falling': falling,\n",
    "        #            'state_dict': model.state_dict(),\n",
    "        #            'optimizer': optimizer.state_dict(),\n",
    "        #            'threshold': threshold\n",
    "        #        }\n",
    "        #            save_ckp(checkpoint, True)\n",
    "        \n",
    "            \n",
    "    #    end = time.time()\n",
    "    #    times[7] += (end-runn)\n",
    "        \n",
    "    #    if before_start > 70*11/batch_size:\n",
    "    #        break\n",
    "    #break\n",
    "    \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "2.045703172683716\n",
      "lr\n",
      "0.0011391120811111322\n",
      "pred\n",
      "tensor([[0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.8000, 0.8000,\n",
      "         0.8000, 0.8000, 0.8000]], device='cuda:0')\n",
      "tensor([[0.4924, 0.5010, 0.5098, 0.5142, 0.5103, 0.5044, 0.4973, 0.5435, 0.5381,\n",
      "         0.5479, 0.5337, 0.5205]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "1.2075682878494263\n",
      "lr\n",
      "0.0011235236238430128\n",
      "pred\n",
      "tensor([[0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.5000, 0.5000, 0.5000,\n",
      "         0.5000, 0.5000, 0.5000]], device='cuda:0')\n",
      "tensor([[0.4927, 0.4951, 0.4946, 0.4824, 0.4941, 0.5049, 0.5059, 0.5000, 0.5151,\n",
      "         0.5063, 0.5117, 0.4954]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "2.2891602516174316\n",
      "lr\n",
      "0.0011079351665748934\n",
      "pred\n",
      "tensor([[0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000,\n",
      "         0.4000, 0.4000, 0.8000]], device='cuda:0')\n",
      "tensor([[0.5820, 0.6016, 0.6055, 0.5796, 0.5640, 0.5923, 0.6309, 0.6270, 0.6611,\n",
      "         0.6299, 0.5991, 0.7837]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "3.256640672683716\n",
      "lr\n",
      "0.001092346709306774\n",
      "pred\n",
      "tensor([[0.7000, 0.7000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000, 0.3000,\n",
      "         0.3000, 0.5000, 0.5000]], device='cuda:0')\n",
      "tensor([[0.8926, 0.8818, 0.6191, 0.5820, 0.6138, 0.6313, 0.5752, 0.5469, 0.6255,\n",
      "         0.6704, 0.7266, 0.6914]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "2.9447264671325684\n",
      "lr\n",
      "0.0010767582520386546\n",
      "pred\n",
      "tensor([[0.6000, 0.6000, 0.6000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000,\n",
      "         0.4000, 0.4000, 0.4000]], device='cuda:0')\n",
      "tensor([[0.8276, 0.8052, 0.8867, 0.6445, 0.6118, 0.6562, 0.6460, 0.6143, 0.5815,\n",
      "         0.6602, 0.6777, 0.7329]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "1.7276854515075684\n",
      "lr\n",
      "0.0010611697947705352\n",
      "pred\n",
      "tensor([[0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.3000, 0.3000, 0.3000,\n",
      "         0.3000, 0.3000, 0.3000]], device='cuda:0')\n",
      "tensor([[0.5996, 0.5967, 0.6616, 0.6387, 0.7676, 0.8149, 0.5205, 0.5020, 0.5122,\n",
      "         0.5259, 0.4919, 0.4893]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "1.3327146768569946\n",
      "lr\n",
      "0.0010455813375024158\n",
      "pred\n",
      "tensor([[0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.4000, 0.4000,\n",
      "         0.4000, 0.4000, 0.4000]], device='cuda:0')\n",
      "tensor([[0.6118, 0.6294, 0.6821, 0.6729, 0.7510, 0.6875, 0.7627, 0.5400, 0.5244,\n",
      "         0.5635, 0.5513, 0.5562]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "2.2122557163238525\n",
      "lr\n",
      "0.0010299928802342964\n",
      "pred\n",
      "tensor([[0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000,\n",
      "         0.7000, 0.3000, 0.3000]], device='cuda:0')\n",
      "tensor([[0.5054, 0.4980, 0.5205, 0.4856, 0.4758, 0.4734, 0.4998, 0.4871, 0.5845,\n",
      "         0.5405, 0.4421, 0.4407]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "1.443603754043579\n",
      "lr\n",
      "0.001014404422966177\n",
      "pred\n",
      "tensor([[0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "         0.6000, 0.6000, 0.4000]], device='cuda:0')\n",
      "tensor([[0.4788, 0.4651, 0.4822, 0.4622, 0.4543, 0.4536, 0.4514, 0.4707, 0.4919,\n",
      "         0.4966, 0.7139, 0.4365]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "1.5291013717651367\n",
      "lr\n",
      "0.0009988159656980577\n",
      "pred\n",
      "tensor([[0.9000, 0.9000, 0.9000, 0.9000, 0.7000, 0.7000, 0.7000, 0.7000, 0.7000,\n",
      "         0.7000, 0.7000, 0.7000]], device='cuda:0')\n",
      "tensor([[0.6548, 0.6460, 0.8848, 0.8193, 0.6021, 0.5825, 0.6587, 0.5635, 0.5259,\n",
      "         0.5400, 0.6079, 0.5854]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "2.4825193881988525\n",
      "lr\n",
      "0.0009832275084299383\n",
      "pred\n",
      "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "         0.6000, 0.6000, 0.6000]], device='cuda:0')\n",
      "tensor([[0.4956, 0.8350, 0.6436, 0.5938, 0.6729, 0.6387, 0.7725, 0.6528, 0.6279,\n",
      "         0.6670, 0.7261, 0.7568]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "1.2980468273162842\n",
      "lr\n",
      "0.0009676390511618182\n",
      "pred\n",
      "tensor([[0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.7000,\n",
      "         0.7000, 0.7000, 0.7000]], device='cuda:0')\n",
      "tensor([[0.8296, 0.8271, 0.7100, 0.7573, 0.8145, 0.7534, 0.9111, 0.9902, 0.8828,\n",
      "         0.8657, 0.7788, 0.7612]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "2.429931640625\n",
      "lr\n",
      "0.0009520505938936979\n",
      "pred\n",
      "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.6000,\n",
      "         0.6000, 0.6000, 0.6000]], device='cuda:0')\n",
      "tensor([[0.4121, 0.4006, 0.4036, 0.4226, 0.4709, 0.7446, 0.4441, 0.4243, 0.7393,\n",
      "         0.7026, 0.6279, 0.6372]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "1.7955565452575684\n",
      "lr\n",
      "0.0009364621366255775\n",
      "pred\n",
      "tensor([[0.1000, 0.1000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000]], device='cuda:0')\n",
      "tensor([[0.3638, 0.3811, 0.9194, 0.9053, 0.8223, 0.7764, 0.6084, 0.6235, 0.6968,\n",
      "         0.7271, 0.9258, 0.9546]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "2.2467775344848633\n",
      "lr\n",
      "0.0009208736793574571\n",
      "pred\n",
      "tensor([[1.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
      "         0.2000, 0.2000, 0.2000]], device='cuda:0')\n",
      "tensor([[0.9609, 0.3093, 0.3110, 0.3154, 0.3149, 0.3486, 0.3411, 0.3643, 0.3892,\n",
      "         0.7100, 0.5327, 0.4712]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "2.432079792022705\n",
      "lr\n",
      "0.0009052852220893368\n",
      "pred\n",
      "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.9000, 0.9000, 0.9000,\n",
      "         0.9000, 0.9000, 0.9000]], device='cuda:0')\n",
      "tensor([[0.3132, 0.3101, 0.3284, 0.3347, 0.3477, 0.3555, 0.8403, 0.8125, 0.8286,\n",
      "         0.7207, 0.5742, 0.5811]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n",
      "loss\n",
      "1.470605492591858\n",
      "lr\n",
      "0.0008896967648212164\n",
      "pred\n",
      "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
      "         0.2000, 0.2000, 0.2000]], device='cuda:0')\n",
      "tensor([[0.6836, 0.7251, 0.9673, 0.8623, 0.9741, 0.2820, 0.2852, 0.2871, 0.2837,\n",
      "         0.3071, 0.3174, 0.3206]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\u001b[39m#loss.backward()\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m startup:\n\u001b[1;32m---> 29\u001b[0m     los[index\u001b[39m%\u001b[39m(\u001b[39m112\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size)] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\u001b[39m/\u001b[39mbatch_size\n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39m%\u001b[39m (\u001b[39m112\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size) \u001b[39m==\u001b[39m (\u001b[39m112\u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:         \n\u001b[0;32m     31\u001b[0m         scaler\u001b[39m.\u001b[39mstep(optimizer)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "startup = True\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    if startup:\n",
    "        training = generateLossImages.MakeIter(start_index=start_index if epoch == start_epoch else 0, startup = True)\n",
    "        training_loader = torch.utils.data.DataLoader(training, batch_size=batch_size, num_workers=2)\n",
    "        min_lr /= batch_size**0.5  # normally one puts the learning rate up by sqrt(batchsize), since that keeps the variance constant, however since we use sum in L1Loss, the oposit is true\n",
    "        max_lr /= batch_size**0.5\n",
    "        step_size = (max_lr-min_lr)/steps\n",
    "        optimizer.param_groups[-1]['lr'] = max_lr\n",
    "        los = [0]*(112//batch_size)\n",
    "    else:\n",
    "        training = generateLossImages.MakeIter(start_index=start_index if epoch == start_epoch else 0, epoch=epoch, startup = False)\n",
    "        training_loader = torch.utils.data.DataLoader(training, batch_size=batch_size, num_workers=2)\n",
    "\n",
    "\n",
    "    for index, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        labels = torch.unsqueeze(labels, dim=-1)\n",
    "        inputs.to(memory_format=torch.channels_last)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(inputs)            \n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()#loss.backward()\n",
    "        if startup:\n",
    "            los[index%(112//batch_size)] = loss.item()/batch_size\n",
    "            if index % (112//batch_size) == (112//batch_size)-1:         \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if falling:\n",
    "            optimizer.param_groups[-1]['lr'] = optimizer.param_groups[-1]['lr'] - step_size\n",
    "            if optimizer.param_groups[-1]['lr'] < min_lr:\n",
    "                falling = False\n",
    "                max_lr *= decay\n",
    "                min_lr *= decay\n",
    "                steps /= decay\n",
    "                step_size = (max_lr-min_lr)/steps\n",
    "                if printing:\n",
    "                    print(\"Saving model !!\")\n",
    "                    print(\"Min lr\")\n",
    "                    print(min_lr)\n",
    "                    print(\"Max lr\")\n",
    "                    print(max_lr)\n",
    "                checkpoint = {'epoch': epoch, 'index': index, 'min_lr': min_lr, 'max_lr': max_lr, 'steps': steps, 'step_size': step_size, 'falling': falling, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'threshold': threshold\n",
    "                }\n",
    "                save_ckp(checkpoint, True)\n",
    "                \n",
    "                if startup and sum(los)/10 < 0.05 and max(los) < 0.1:\n",
    "                    startup = False\n",
    "                    max_lr /= 4\n",
    "                    min_lr /= 4\n",
    "                    steps *= 2\n",
    "                    step_size = (max_lr-min_lr)/steps\n",
    "                    falling = True\n",
    "                    optimizer.param_groups[-1]['lr'] = max_lr\n",
    "                    print(\"startup done ! !\")\n",
    "                    break\n",
    "        else: \n",
    "            optimizer.param_groups[-1]['lr'] += step_size\n",
    "            if optimizer.param_groups[-1]['lr'] > max_lr:\n",
    "                falling = True\n",
    "\n",
    "        if (startup and printing and index % (112//batch_size) == (112//batch_size)-1) or (not startup and printing and index % (20*(112//batch_size) == (112//batch_size))-1):\n",
    "            print(\"loss\")\n",
    "            print(loss.item())\n",
    "            print(\"lr\")\n",
    "            print(optimizer.param_groups[-1]['lr'])\n",
    "            print(\"pred\")\n",
    "            print(labels.T)\n",
    "            print(outputs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0040051937103271484, 0.8806514739990234, 0.0, 0.1621570587158203, 0.001001119613647461, 0.31227612495422363, 0.0, 0.024025678634643555]\n",
      "[0.2893682198971097, 63.62552420967629, 0.0, 11.715563045892052, 0.07232913598118719, 22.561402251625466, 0.0, 1.735813136927891]\n"
     ]
    }
   ],
   "source": [
    "print(times)\n",
    "su = sum(times)\n",
    "print([100*l/su for l in times])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4193603606135756\n"
     ]
    }
   ],
   "source": [
    "print(su/15*6645*50*4/86400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f33075560525c4759195768b52de2817c211ae9aef6733cbe5230884af85532a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
