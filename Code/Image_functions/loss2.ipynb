{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#from torch import nn\n",
    "#import numpy as np\n",
    "#from torchsummary import summary\n",
    "#import Lossv2\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#summary(Lossv2.Loss().to(device), (6, 768, 512))  ###  IDK why, but I have to switch the dims in AvgPool on Loss() to get this to work :/ But that breaks the stuff bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "torch.manual_seed(100)\n",
    "np.random.seed(100)\n",
    "import Lossv2\n",
    "import generateLossImages\n",
    "import time\n",
    "\n",
    "global counter \n",
    "counter = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def save_ckp(state, is_best, checkpoint_dir=\"./models/rest/\", best_model_dir=\"./models/best/\"):\n",
    "    global counter \n",
    "    f_path = checkpoint_dir + str(counter) + '_checkpoint.pt'\n",
    "    counter = counter + 1\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath = best_model_dir + 'best_model.pt'\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "\n",
    "def load_ckp(model, optimizer, checkpoint_fpath=\"./models/best/best_model.pt\"):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch'], checkpoint['index'], checkpoint['min_lr'], checkpoint['max_lr'], checkpoint['steps'], checkpoint['step_size'], checkpoint['falling'], checkpoint['threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "loss_fn = nn.L1Loss()\n",
    "min_lr = 0.0003\n",
    "max_lr = 0.001\n",
    "decay = 0.9\n",
    "steps = 500\n",
    "falling = True\n",
    "start_epoch = 0\n",
    "start_index = 0\n",
    "momentum = 0.94\n",
    "threshold = [0.16, 0.12, 0.09, 0.06, 0.04, 0.03, 0.02, 0.015, 0.01 -1]\n",
    "step_size = (max_lr-min_lr)/steps\n",
    "model = Lossv2.Loss().to(device).to(memory_format=torch.channels_last)\n",
    "#x = torch.rand(22, 8, 512, 768).to(device).to(memory_format=torch.channels_last)\n",
    "model = torch.jit.script(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=max_lr, momentum=momentum)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "#\n",
    "#torch.autograd.set_detect_anomaly(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn = nn.L1Loss()\n",
    "#model = Lossv2.Loss().to(device)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.96)\n",
    "#model, optimizer, start_epoch, start_index, min_lr, max_lr, steps, step_size, falling, threshold = load_ckp(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#li = [31.007527351379395, 0.01501011848449707, 0.0010006427764892578, 28.98952603340149, 0.48702239990234375, 47.146605014801025, 12.32689380645752, 0.5665144920349121]\n",
    "#su = sum(li)\n",
    "#print([100*l/su for l in li])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rani\\AppData\\Local\\Temp\\ipykernel_19104\\2895240763.py:17: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  torch.autograd.detect_anomaly(check_nan=False)\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "printing = True\n",
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "lis = []\n",
    " # might have to delete when loading model\n",
    "threshold_decay = 0.2\n",
    "flags = [True for _ in threshold]\n",
    "record = 1\n",
    "epochs = 100\n",
    "times = [0]*8\n",
    "batch_size= 22\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.autograd.detect_anomaly(check_nan=False)\n",
    "torch.autograd.set_detect_anomaly(False, check_nan=False)\n",
    "torch.autograd.profiler.profile(enabled=False)\n",
    "torch.autograd.profiler.emit_nvtx(enabled=False)\n",
    "#torch.autograd.gradgradcheck(check_undefined_grad= False)\n",
    "#torch.autograd.gradcheck(check_undefined_grad= False)\n",
    "#torch.no_grad()\n",
    "\n",
    "scaler = GradScaler()\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "0.41122591495513916\n",
      "lr\n",
      "0.0009860000000000008\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5806],\n",
      "        [1.0000],\n",
      "        [0.7974],\n",
      "        [0.9849],\n",
      "        [0.8696],\n",
      "        [1.0000],\n",
      "        [0.5796],\n",
      "        [1.0000],\n",
      "        [0.9985],\n",
      "        [0.6792],\n",
      "        [0.9331],\n",
      "        [0.6143],\n",
      "        [1.0000],\n",
      "        [0.6299],\n",
      "        [0.9883],\n",
      "        [0.8896],\n",
      "        [1.0000],\n",
      "        [0.5796],\n",
      "        [1.0000],\n",
      "        [0.9995],\n",
      "        [0.6240],\n",
      "        [0.9473]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.2733413875102997\n",
      "lr\n",
      "0.0005659999999999912\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5000],\n",
      "        [0.5195],\n",
      "        [0.4985],\n",
      "        [0.4834],\n",
      "        [0.4910],\n",
      "        [0.4861],\n",
      "        [0.4976],\n",
      "        [0.5186],\n",
      "        [0.4958],\n",
      "        [0.4993],\n",
      "        [0.4888],\n",
      "        [0.4985],\n",
      "        [0.5107],\n",
      "        [0.4976],\n",
      "        [0.4841],\n",
      "        [0.4851],\n",
      "        [0.4861],\n",
      "        [0.4980],\n",
      "        [0.5142],\n",
      "        [0.4958],\n",
      "        [0.4998],\n",
      "        [0.4849]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "0.00027\n",
      "0.0009000000000000001\n",
      "555.5555555555555\n",
      "1.1340000000000003e-06\n",
      "loss\n",
      "0.2737661302089691\n",
      "lr\n",
      "0.00042473999999999426\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5073],\n",
      "        [0.5801],\n",
      "        [0.5049],\n",
      "        [0.5059],\n",
      "        [0.5059],\n",
      "        [0.5122],\n",
      "        [0.5020],\n",
      "        [0.5728],\n",
      "        [0.5381],\n",
      "        [0.5024],\n",
      "        [0.5093],\n",
      "        [0.5015],\n",
      "        [0.5117],\n",
      "        [0.5015],\n",
      "        [0.5044],\n",
      "        [0.5029],\n",
      "        [0.5103],\n",
      "        [0.5020],\n",
      "        [0.5098],\n",
      "        [0.5054],\n",
      "        [0.5034],\n",
      "        [0.5020]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.2734029293060303\n",
      "lr\n",
      "0.0007649399999999871\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.4958],\n",
      "        [0.4858],\n",
      "        [0.4937],\n",
      "        [0.4912],\n",
      "        [0.4854],\n",
      "        [0.4937],\n",
      "        [0.4961],\n",
      "        [0.4792],\n",
      "        [0.4790],\n",
      "        [0.4978],\n",
      "        [0.4819],\n",
      "        [0.4971],\n",
      "        [0.4878],\n",
      "        [0.4951],\n",
      "        [0.4922],\n",
      "        [0.4895],\n",
      "        [0.4919],\n",
      "        [0.4968],\n",
      "        [0.4905],\n",
      "        [0.4883],\n",
      "        [0.4961],\n",
      "        [0.4875]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.27323344349861145\n",
      "lr\n",
      "0.0006968999999999891\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.4978],\n",
      "        [0.5205],\n",
      "        [0.5020],\n",
      "        [0.4998],\n",
      "        [0.5034],\n",
      "        [0.5078],\n",
      "        [0.5010],\n",
      "        [0.5210],\n",
      "        [0.5098],\n",
      "        [0.5020],\n",
      "        [0.5039],\n",
      "        [0.5015],\n",
      "        [0.5122],\n",
      "        [0.5020],\n",
      "        [0.5205],\n",
      "        [0.5015],\n",
      "        [0.5352],\n",
      "        [0.5015],\n",
      "        [0.5122],\n",
      "        [0.5034],\n",
      "        [0.5020],\n",
      "        [0.5010]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.2727340757846832\n",
      "lr\n",
      "0.00035669999999999306\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5000],\n",
      "        [0.5020],\n",
      "        [0.4995],\n",
      "        [0.4976],\n",
      "        [0.4963],\n",
      "        [0.4946],\n",
      "        [0.4993],\n",
      "        [0.5059],\n",
      "        [0.5020],\n",
      "        [0.4980],\n",
      "        [0.4968],\n",
      "        [0.5005],\n",
      "        [0.4966],\n",
      "        [0.5005],\n",
      "        [0.4966],\n",
      "        [0.4998],\n",
      "        [0.4968],\n",
      "        [0.5000],\n",
      "        [0.4956],\n",
      "        [0.4985],\n",
      "        [0.5000],\n",
      "        [0.5010]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "0.000243\n",
      "0.0008100000000000001\n",
      "617.283950617284\n",
      "9.1854e-07\n",
      "loss\n",
      "0.2728632092475891\n",
      "lr\n",
      "0.00047421641999998945\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.4988],\n",
      "        [0.5142],\n",
      "        [0.5039],\n",
      "        [0.5015],\n",
      "        [0.5010],\n",
      "        [0.5034],\n",
      "        [0.5020],\n",
      "        [0.5132],\n",
      "        [0.5039],\n",
      "        [0.5015],\n",
      "        [0.5005],\n",
      "        [0.5015],\n",
      "        [0.5073],\n",
      "        [0.5029],\n",
      "        [0.5005],\n",
      "        [0.5000],\n",
      "        [0.5005],\n",
      "        [0.5029],\n",
      "        [0.5103],\n",
      "        [0.5029],\n",
      "        [0.5020],\n",
      "        [0.5000]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.27278855443000793\n",
      "lr\n",
      "0.0007497784200000021\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5010],\n",
      "        [0.4937],\n",
      "        [0.4980],\n",
      "        [0.4976],\n",
      "        [0.4983],\n",
      "        [0.4973],\n",
      "        [0.5005],\n",
      "        [0.4941],\n",
      "        [0.4963],\n",
      "        [0.4995],\n",
      "        [0.4971],\n",
      "        [0.5020],\n",
      "        [0.4961],\n",
      "        [0.5010],\n",
      "        [0.4954],\n",
      "        [0.4995],\n",
      "        [0.4937],\n",
      "        [0.4998],\n",
      "        [0.4956],\n",
      "        [0.4961],\n",
      "        [0.4998],\n",
      "        [0.4978]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.27297115325927734\n",
      "lr\n",
      "0.0005954636999999946\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.4993],\n",
      "        [0.4934],\n",
      "        [0.5005],\n",
      "        [0.4883],\n",
      "        [0.5010],\n",
      "        [0.4846],\n",
      "        [0.5005],\n",
      "        [0.4949],\n",
      "        [0.4963],\n",
      "        [0.4998],\n",
      "        [0.5005],\n",
      "        [0.5000],\n",
      "        [0.4866],\n",
      "        [0.4995],\n",
      "        [0.4951],\n",
      "        [0.4983],\n",
      "        [0.4924],\n",
      "        [0.5015],\n",
      "        [0.4858],\n",
      "        [0.4902],\n",
      "        [0.5005],\n",
      "        [0.4968]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.27314871549606323\n",
      "lr\n",
      "0.00031990169999999106\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5020],\n",
      "        [0.4775],\n",
      "        [0.5000],\n",
      "        [0.4915],\n",
      "        [0.4951],\n",
      "        [0.4885],\n",
      "        [0.5005],\n",
      "        [0.4800],\n",
      "        [0.4868],\n",
      "        [0.4998],\n",
      "        [0.4934],\n",
      "        [0.5005],\n",
      "        [0.4863],\n",
      "        [0.4988],\n",
      "        [0.4956],\n",
      "        [0.4961],\n",
      "        [0.4937],\n",
      "        [0.5005],\n",
      "        [0.4873],\n",
      "        [0.4929],\n",
      "        [0.4988],\n",
      "        [0.4954]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "0.0002187\n",
      "0.000729\n",
      "685.8710562414266\n",
      "7.440174e-07\n",
      "loss\n",
      "0.2726937234401703\n",
      "lr\n",
      "0.0004034520983999943\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5034],\n",
      "        [0.4985],\n",
      "        [0.5000],\n",
      "        [0.4978],\n",
      "        [0.4983],\n",
      "        [0.4978],\n",
      "        [0.4993],\n",
      "        [0.4966],\n",
      "        [0.4971],\n",
      "        [0.4995],\n",
      "        [0.4985],\n",
      "        [0.5005],\n",
      "        [0.4978],\n",
      "        [0.5054],\n",
      "        [0.5000],\n",
      "        [0.4993],\n",
      "        [0.4988],\n",
      "        [0.5010],\n",
      "        [0.4978],\n",
      "        [0.4985],\n",
      "        [0.4993],\n",
      "        [0.4995]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.272796630859375\n",
      "lr\n",
      "0.0006266573183999877\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5029],\n",
      "        [0.5122],\n",
      "        [0.5005],\n",
      "        [0.5054],\n",
      "        [0.5015],\n",
      "        [0.5093],\n",
      "        [0.5005],\n",
      "        [0.5103],\n",
      "        [0.5073],\n",
      "        [0.4995],\n",
      "        [0.5024],\n",
      "        [0.5010],\n",
      "        [0.5005],\n",
      "        [0.4998],\n",
      "        [0.4988],\n",
      "        [0.5000],\n",
      "        [0.5010],\n",
      "        [0.5005],\n",
      "        [0.4993],\n",
      "        [0.5015],\n",
      "        [0.4995],\n",
      "        [0.4990]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.27267152070999146\n",
      "lr\n",
      "0.0006088009007999887\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.5010],\n",
      "        [0.5005],\n",
      "        [0.4995],\n",
      "        [0.4978],\n",
      "        [0.4988],\n",
      "        [0.4971],\n",
      "        [0.4998],\n",
      "        [0.4993],\n",
      "        [0.4973],\n",
      "        [0.4995],\n",
      "        [0.4976],\n",
      "        [0.4993],\n",
      "        [0.4983],\n",
      "        [0.4990],\n",
      "        [0.4978],\n",
      "        [0.4993],\n",
      "        [0.4968],\n",
      "        [0.4993],\n",
      "        [0.4971],\n",
      "        [0.4988],\n",
      "        [0.5000],\n",
      "        [0.4988]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "loss\n",
      "0.2725968658924103\n",
      "lr\n",
      "0.000385595680799994\n",
      "pred\n",
      "tensor([0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000, 0.9000, 0.7000,\n",
      "        0.3000, 0.5000, 0.0010, 0.9990, 0.2000, 0.6000, 0.4000, 0.8000, 0.1000,\n",
      "        0.9000, 0.7000, 0.3000, 0.5000], device='cuda:0')\n",
      "tensor([[0.4993],\n",
      "        [0.4983],\n",
      "        [0.4995],\n",
      "        [0.4993],\n",
      "        [0.4998],\n",
      "        [0.4998],\n",
      "        [0.4998],\n",
      "        [0.4973],\n",
      "        [0.4990],\n",
      "        [0.4993],\n",
      "        [0.4998],\n",
      "        [0.5000],\n",
      "        [0.5005],\n",
      "        [0.5000],\n",
      "        [0.4993],\n",
      "        [0.4998],\n",
      "        [0.5005],\n",
      "        [0.4998],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.4995],\n",
      "        [0.4993]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "0.00019683\n",
      "0.0006561000000000001\n",
      "762.0789513793628\n",
      "6.026540940000002e-07\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: './models/best/best_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 78\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[39mprint\u001b[39m(step_size)\n\u001b[0;32m     66\u001b[0m         checkpoint \u001b[39m=\u001b[39m {\n\u001b[0;32m     67\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch,\n\u001b[0;32m     68\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m: index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mthreshold\u001b[39m\u001b[39m'\u001b[39m: threshold\n\u001b[0;32m     77\u001b[0m         }\n\u001b[1;32m---> 78\u001b[0m         save_ckp(checkpoint, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     80\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[0;32m     81\u001b[0m     optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m step_size\n",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36msave_ckp\u001b[1;34m(state, is_best, checkpoint_dir, best_model_dir)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m is_best:\n\u001b[0;32m     21\u001b[0m     best_fpath \u001b[39m=\u001b[39m best_model_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbest_model.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 22\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopyfile(f_path, best_fpath)\n",
      "File \u001b[1;32mc:\\Users\\Rani\\anaconda3\\envs\\torch\\lib\\shutil.py:266\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(src, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fsrc:\n\u001b[0;32m    265\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(dst, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[0;32m    267\u001b[0m             \u001b[39m# macOS\u001b[39;00m\n\u001b[0;32m    268\u001b[0m             \u001b[39mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[0;32m    269\u001b[0m                 \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: './models/best/best_model.pt'"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    training = generateLossImages.MakeIter(start_index=start_index if epoch == start_epoch else 0, epoch=epoch) # start=2*71*50*4))#start_index if epoch == start_epoch else 0)) \n",
    "    #val = generateLossImages.get_image_pairs_transforms_with_loss(\"C:/Users/Rani/Desktop/ai_val/16\")\n",
    "    training_loader = torch.utils.data.DataLoader(training, batch_size=batch_size, num_workers=2)#), worker_init_fn=worker_init_fn) # num_workers\n",
    "    end = 0\n",
    "    before_start = 0\n",
    "    in_upstart = True\n",
    "    for index, data in enumerate(training_loader):#[90.06174230575562, 0.03202486038208008, 0.016014575958251953, 84.36967897415161, 1.3647308349609375, 137.4005196094513, 34.81299662590027, 1.9427943229675293, 0, 0]\n",
    "        #print(len(inputs))\n",
    "        before_start += 1\n",
    "        #if before_start > 55*11/batch_size and in_upstart:\n",
    "            #in_upstart = False\n",
    "            #times = [0]*8\n",
    "        #start = time.time()\n",
    "        #if end != 0:\n",
    "            #times[0] += (start-end)\n",
    "        inputs, labels = data\n",
    "        #labels = torch.unsqueeze(labels, dim=-1)\n",
    "        inputs.to(memory_format=torch.channels_last)\n",
    "        #load = time.time()\n",
    "        #times[1] += load-start\n",
    "\n",
    "        #lab = time.time()\n",
    "        #times[2] += (lab-load)\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #out = time.time()\n",
    "            #times[3] += (out-lab)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            #los = time.time()\n",
    "            #times[4] += (los-out)\n",
    "\n",
    "        loss.backward()#scaler.scale(loss).backward()#loss.backward()\n",
    "        \n",
    "       # back = time.time()\n",
    "        #times[5] += (back-los)\n",
    "        # Gather data and report\n",
    "        #running_loss += loss.item()\n",
    "        \n",
    "        #runn = time.time()\n",
    "        #times[6] += (runn-back)\n",
    "        optimizer.step()#scaler.step(optimizer)#optimizer.step()\n",
    "        #scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if falling:\n",
    "            optimizer.param_groups[-1]['lr'] = optimizer.param_groups[-1]['lr'] - step_size\n",
    "            if optimizer.param_groups[-1]['lr'] < min_lr:\n",
    "                falling = False\n",
    "                max_lr *= decay\n",
    "                min_lr *= decay\n",
    "                steps /= decay\n",
    "                step_size = (max_lr-min_lr)/steps\n",
    "\n",
    "                if printing:\n",
    "                    print(min_lr)\n",
    "                    print(max_lr)\n",
    "                    print(steps)\n",
    "                    print(step_size)\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'index': index,\n",
    "                    'min_lr': min_lr,\n",
    "                    'max_lr': max_lr,\n",
    "                    'steps': steps,\n",
    "                    'step_size': step_size,\n",
    "                    'falling': falling,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'threshold': threshold\n",
    "                }\n",
    "                save_ckp(checkpoint, True)\n",
    "\n",
    "        else: \n",
    "            optimizer.param_groups[-1]['lr'] += step_size\n",
    "            if optimizer.param_groups[-1]['lr'] > max_lr:\n",
    "                falling = True\n",
    "\n",
    "        if printing and index % 300 == 9:\n",
    "            print(\"loss\")\n",
    "            print(loss.item())\n",
    "            print(\"lr\")\n",
    "            print(optimizer.param_groups[-1]['lr'])\n",
    "            print(\"pred\")\n",
    "            print(labels)\n",
    "            print(outputs)\n",
    "        #        print('  batch {} loss: {}'.format(index + 1, last_loss))\n",
    "        #        print(lis)\n",
    "        #        lis = []\n",
    "        #    running_loss = 0\n",
    "\n",
    "        #    if last_loss < record:\n",
    "        #        record = last_loss\n",
    "        #        if last_loss < 0.08:\n",
    "        #            checkpoint = {\n",
    "        #            'epoch': epoch,\n",
    "        #            'index': index,\n",
    "        #            'min_lr': min_lr,\n",
    "        #            'max_lr': max_lr,\n",
    "        #            'steps': steps,\n",
    "        #            'step_size': step_size,\n",
    "        #            'falling': falling,\n",
    "        #            'state_dict': model.state_dict(),\n",
    "        #            'optimizer': optimizer.state_dict(),\n",
    "        #            'threshold': threshold\n",
    "        #        }\n",
    "        #            save_ckp(checkpoint, True)\n",
    "        #   print(times)\n",
    "            \n",
    "        #end = time.time()\n",
    "        #times[7] += (end-runn)\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.004003763198852539, 0.04904508590698242, 0.0, 0.16615056991577148, 0.001001119613647461, 1.1347110271453857, 0.0, 0.027023792266845703]\n",
      "[0.2897214529997548, 3.5490144760661924, 0.0, 12.02303486543126, 0.07244330263478656, 82.11028254958329, 0.0, 1.9555033532847146]\n"
     ]
    }
   ],
   "source": [
    "print(times)\n",
    "su = sum(times)\n",
    "print([100*l/su for l in times])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4171235268866573\n"
     ]
    }
   ],
   "source": [
    "print(su/15*6645*50*4/86400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
